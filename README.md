# python-project6 [make_translator]
## <한영번역기, seq2seq with attention>

이번에 한영번역기를 만들어보았다.(사실 내가 한거라고는 85% 완성된거에 15% 붙인거지만...) 
최종으로는 하나의 ipynb파일이 올라가지만, 이걸 하기위해 4개의 ipynb파일을 만들어서 여러가지 실험을 해봤다... 여기에는 그 실험기를 적을 예정이다.
향후 이 파일을 모듈화해서 깃헙에 올릴 예정이다.

아, 최종적으로 올라간 파일의 BLEU점수가 제일 높은건 아니다. 그냥 가장 깔끔하게 코드를 구현하고 설명해서 가져옴.
(팀원 분 점수가 사실 제일 높았던 것은 안 비밀)

import한 파일 : 한국어-영어 번역(병렬) 말뭉치 중 3개 파일(구어체(1), 구어체(2), 대화체) 총 50만 문장. [출처 : AI Hub(https://aihub.or.kr/)]
* 여기서 데이터를 받으려면 가입 후 인증 받아서 신청해야함. 다양한 데이터가 있으므로 둘러보는것도 좋을 듯.

### 전처리 및 토크나이징

사실 AIhub의 대부분의 파일들은 문장이 깔끔하게 정렬도 되어있고 전처리할 필요가 거의 없으나, 번역시에 반영이 안될수도 있거나 깨질수도 있는 문장이 있을 수 있어 그거를 전처리하는데 집중함.

1. unicode_hangeul 함수

<img width="1101" alt="스크린샷 2023-03-23 오후 8 09 50" src="https://user-images.githubusercontent.com/121400054/227186172-3f155482-7efc-4139-9a30-9d94dedfa917.png">

한글을 뗐다 붙이는 작업을 한다. 
가끔 한글이 예를 들어 '한글'이 아니라 'ㅎㅏㄴㄱㅡㄹ'이렇게 보이거나, 설령 '한글'로 잘 보이더라고 막상 문자열 길이(len)를 출력하면 2가 아닌 6이 나오는 경우가 있다.
그런 경우 뒤에 time_step이나 vocab_size에 영향을 줄 수 있어, 글자를 쪼갰다가 붙이는 함수로 가져왔다.
NFKC가 아니라 다른 코드도 있으니 찾아보면 좋다.

2. preprocess_sentence 함수

<img width="790" alt="스크린샷 2023-03-23 오후 8 14 55" src="https://user-images.githubusercontent.com/121400054/227188166-7a4f8f91-dcdf-4d40-8bf2-6aa13862c945.png">

한글 전처리 함수, 구두점 사이에 공백을 만드는 이유는 구두점을 토큰으로 분류하여 문장의 경계를 알 수 있도록 하기 위함이다.
(출처 : https://wikidocs.net/21698)
그 후 번역이 될 숫자, 영어, 한글, 몇개의 특수문자 등을 제외하고는 정규표현식으로 처리해준다.
마지막에 lower 객체를 불러오는 이유는, 대문자를 없앰으로써 향후 토크나이저에 피팅시 차원을 축소하기 위함이다.

3. 토크나이저
이 부분에서 사실 상당히 고민했다. 어떻게 문장을 잘라야 잘 인식을 해서 벡터로 변환이 되고, 그 받은 벡터를 잘 인식해서 다시 텍스트로 변환할지 고민이 됐기 때문이다.
이 부분에서 3가지의 토크나이저를 이용해보았다.(텐서플로 토크나이저 포함하면 4가지) 
최종적으로는 한글은 HuggingFace Tokenizer를 이용해서 자른 다음 텐서플로 토크나이저에 fit하였고, 영어는 그냥 공백을 기준으로 자른 다음 텐서플로 토크나이저에 fit하였다.
아, 한글은 왜 그냥 공백 기준으로 안잘랐냐면, 텐서플로 토크나이저에 fit했을 때 단어뭉치가 20만 문장기준 19만개로 어마어마하게 많이 나왔기 때문이다. 영어 문장은 단어 뭉치가 2만개였던거에 비해서.

1. 한글 - mecab tokenizer


