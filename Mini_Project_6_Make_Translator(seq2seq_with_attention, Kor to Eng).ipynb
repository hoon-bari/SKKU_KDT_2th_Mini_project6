{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리 및 토크나이징"
      ],
      "metadata": {
        "id": "UkpAREsJ_Grn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Softmax, Dropout, Attention"
      ],
      "metadata": {
        "id": "i2MZuIPE_MZ0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('/content/drive/MyDrive/한국어-영어 번역(병렬) 말뭉치/1_구어체(1).xlsx')\n",
        "df1 = df1[['원문','번역문']]\n",
        "df1"
      ],
      "metadata": {
        "id": "hIXCdcTE_OZB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_excel('/content/drive/MyDrive/한국어-영어 번역(병렬) 말뭉치/1_구어체(2).xlsx')\n",
        "df2 = df2[['원문','번역문']]\n",
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XThYJGI5_W8Q",
        "outputId": "448f67ec-4a33-4b07-df77-9d1667c7341c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.read_excel('/content/drive/MyDrive/한국어-영어 번역(병렬) 말뭉치/2_대화체.xlsx')\n",
        "df3 = df3[['원문','번역문']]\n",
        "df3"
      ],
      "metadata": {
        "id": "sSCDSwLe_Zs6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터\n",
        "BATCH_SIZE = 256 # Batch size for training.\n",
        "EPOCHS = 10  # Number of epochs to train for.\n",
        "HIDDEN_DIM = 256  # Latent dimensionality of the encoding space.\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_SAMPLES = 500000  # Number of samples to train on."
      ],
      "metadata": {
        "id": "vouKjfrw_shC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset \n",
        "hangeul_corpus, eng_corpus = [], []\n",
        "for row1, row2 in zip(df1.itertuples(), df2.itertuples()):\n",
        "    # source 데이터와 target 데이터 분리\n",
        "    hangeul1, eng1 = row1.원문, row1.번역문\n",
        "    hangeul2, eng2 = row2.원문, row2.번역문\n",
        "    \n",
        "    hangeul_corpus.extend([hangeul1, hangeul2])\n",
        "    eng_corpus.extend([eng1, eng2])\n",
        "\n",
        "for row3 in df3.itertuples():\n",
        "    hangeul3, eng3 = row3.원문, row3.번역문\n",
        "\n",
        "    hangeul_corpus.append(hangeul3)\n",
        "    eng_corpus.append(eng3)\n",
        "    \n",
        "print(len(hangeul_corpus), hangeul_corpus[:10])\n",
        "print(len(eng_corpus), eng_corpus[:10])"
      ],
      "metadata": {
        "id": "hWdU1yX3_2Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_hangeul(s):\n",
        "    # 한글 뗐다 붙이기 -> 컴퓨터가 인식하는 문자열의 길이를 같게 만들어주기 위함\n",
        "    return unicodedata.normalize('NFKC', s)"
      ],
      "metadata": {
        "id": "5uSaPorBAOsa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sent):\n",
        "    # 한글 전처리 함수\n",
        "    sent = unicode_hangeul(sent)\n",
        "\n",
        "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "    sent = re.sub(r\"([?.!])\", r\" \\1\", sent)\n",
        "\n",
        "    # (a-z, A-Z, 가-힣, 0-9, \".\", \"?\", \"!\", \",\", \"'\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "    sent = re.sub(r\"[^a-zA-Z가-힣ㄱ-ㅎ0-9!.?']+\", r\" \", sent)\n",
        "\n",
        "    # 다수 개의 공백을 하나의 공백으로 치환\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    return sent.lower()"
      ],
      "metadata": {
        "id": "6cl8MYV6ASBv"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface tokenizer 써봄 -> 한글만, 영어는 그냥 텐서플로 토크나이저로\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "2je6G10VATwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "metadata": {
        "id": "4uZ7DKdHAZZE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface tokenizer는 먼저 학습을 시켜야함. 원문과 번역문을 각각 txt파일로 저장\n",
        "with open('make_translator[kor].txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(pd.Series(hangeul_corpus)))"
      ],
      "metadata": {
        "id": "ff7Wy3MbAbA8"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hug_kor_tok = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
      ],
      "metadata": {
        "id": "-yvNxKeABJNb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hug_kor_tok.train(files='/content/make_translator[kor].txt', vocab_size=60000, limit_alphabet=10000, min_frequency=5)"
      ],
      "metadata": {
        "id": "YSnj_I4pBs3G"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data(hangeul_corpus, eng_corpus): # 여기서 바로 전처리 후 토크나이징(한글), 영어는 띄어쓰기로만\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "    for i, (src_line, tar_line) in enumerate(zip(hangeul_corpus, eng_corpus)):\n",
        "        src_line = preprocess_sentence(src_line)\n",
        "        src_encoded = hug_kor_tok.encode(src_line)\n",
        "        src_encoded = src_encoded.tokens\n",
        "\n",
        "        tar_line = preprocess_sentence(tar_line)\n",
        "        tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "        tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "        encoder_input.append(src_encoded) # 클리닝 \n",
        "        decoder_input.append(tar_line_in) # 클리닝 + sos \n",
        "        decoder_target.append(tar_line_out) # 클리닝 + eos \n",
        "\n",
        "        if i == NUM_SAMPLES - 1:\n",
        "            break\n",
        "                    \n",
        "    return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "3e_fWU1VB1JC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_hangeul_in, sents_en_in, sents_en_out  = load_preprocessed_data(hangeul_corpus, eng_corpus)"
      ],
      "metadata": {
        "id": "sSsa6Nl4Cj4C"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력 :',sents_hangeul_in[-5:])\n",
        "print('디코더의 입력 :',sents_en_in[-5:])\n",
        "print('디코더의 레이블 :',sents_en_out[-5:])"
      ],
      "metadata": {
        "id": "GZNfGYBHCm1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 순서처리로 텐서플로 토크나이저 이용, 한글\n",
        "tokenizer_kor = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_kor.fit_on_texts(sents_hangeul_in)\n",
        "encoder_input = tokenizer_kor.texts_to_sequences(sents_hangeul_in)"
      ],
      "metadata": {
        "id": "sI_VzWrJC13r"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 순서처리로 텐서플로 토크나이저 이용, 영어\n",
        "tokenizer_eng = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_eng.fit_on_texts(sents_en_in)\n",
        "tokenizer_eng.fit_on_texts(sents_en_out)\n",
        "\n",
        "# 디코더 데이터\n",
        "decoder_input = tokenizer_eng.texts_to_sequences(sents_en_in)\n",
        "decoder_target = tokenizer_eng.texts_to_sequences(sents_en_out)"
      ],
      "metadata": {
        "id": "cWZUjX3BDOg_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = len(tokenizer_kor.word_index) + 1\n",
        "TAR_VOCAB_SIZE = len(tokenizer_eng.word_index) + 1\n",
        "\n",
        "print(f\"한글 단어 집합의 크기 : {SRC_VOCAB_SIZE}, 영어 단어 집합의 크기 : {TAR_VOCAB_SIZE}\")"
      ],
      "metadata": {
        "id": "jz0aIVPJDRCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, padding='post')"
      ],
      "metadata": {
        "id": "rweHGlnWDeVb"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
        "\n",
        "# 변수 저장\n",
        "MAX_ENC_LEN, MAX_DEC_LEN = encoder_input.shape[1], decoder_input.shape[1]"
      ],
      "metadata": {
        "id": "TEfMHwhKDhm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src2idx = tokenizer_kor.word_index  # word : idx\n",
        "idx2src = tokenizer_kor.index_word  # idx : word\n",
        "tar2idx = tokenizer_eng.word_index # word : idx\n",
        "idx2tar = tokenizer_eng.index_word # idx : word"
      ],
      "metadata": {
        "id": "LAj3huMFDj_X"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tar2idx)\n",
        "print(idx2tar)"
      ],
      "metadata": {
        "id": "fnpTGO7kOscD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤 인덱스 생성 \n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)\n",
        "\n",
        "# 랜덤하게 섞기\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "print(encoder_input[indices][0])\n",
        "print(decoder_input[indices[0]])\n",
        "print(decoder_target[indices[0]])"
      ],
      "metadata": {
        "id": "MzdD6V6KDmHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(NUM_SAMPLES*0.1) # 20000\n",
        "\n",
        "# train data\n",
        "encoder_input_train = encoder_input[:-n_of_val] \n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "# test data\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "fOUMjBjnDoaw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "id": "8NWHKzSrDqvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "# input, layer\n",
        "encoder_inputs = Input(shape = (MAX_ENC_LEN,))\n",
        "enc_emb_layer = Embedding(SRC_VOCAB_SIZE, EMBEDDING_DIM, name='ENC_Embedding')\n",
        "enc_dropout = Dropout(0.2, name='ENC_Dropout')\n",
        "enc_lstm = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True, name='ENC_LSTM')\n",
        "\n",
        "# graph\n",
        "enc_emb = enc_emb_layer(encoder_inputs)\n",
        "enc_emb = enc_dropout(enc_emb)\n",
        "encoder_outputs, enc_h, enc_c = enc_lstm(enc_emb)\n",
        "encoder_states = [enc_h, enc_c]"
      ],
      "metadata": {
        "id": "LekVMMEnDr8e"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "# input, layer\n",
        "decoder_inputs = Input(shape = (MAX_DEC_LEN,))\n",
        "dec_emb_layer = Embedding(TAR_VOCAB_SIZE, EMBEDDING_DIM, name='DEC_Embedding')\n",
        "dec_dropout = Dropout(0.2, name='DEC_Dropout')\n",
        "dec_lstm = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True, name='DEC_LSTM')\n",
        "att = Attention()\n",
        "dense_tanh = Dense(HIDDEN_DIM, activation = 'tanh')\n",
        "dec_dense = Dense(TAR_VOCAB_SIZE, activation='softmax', name='DEC_Dense')\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "dec_emb = dec_dropout(dec_emb)\n",
        "decoder_output_, dec_h, dec_c = dec_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# 어텐션을 구현해보자\n",
        "# attention_score = tf.matmul(dec_h, encoder_outputs, transpose_b=True)\n",
        "# attention_weight = tf.nn.softmax(attention_score)\n",
        "# attention_values = tf.matmul(attention_weight, encoder_outputs)\n",
        "\n",
        "# 어텐션 클래스를 사용해보자\n",
        "context_vector = att([decoder_output_, encoder_outputs])\n",
        "concat = dense_tanh(Concatenate(axis=-1)([context_vector, decoder_output_]))\n",
        "decoder_outputs = dec_dense(concat)"
      ],
      "metadata": {
        "id": "tS5TRgz5DwCP"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "QQt7Q0MgDzA4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "UMS2YBdAD2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=3)"
      ],
      "metadata": {
        "id": "QFuHxA0eD3-p"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트로 현재 모델의 베스트 weight 저장\n",
        "checkpoint_path = '/content/drive/MyDrive/checkpoint.h5'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True, \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1\n",
        "                            )"
      ],
      "metadata": {
        "id": "sq2l4hSXD5bG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연속하여 학습시 체크포인트를 로드하여 이어서 학습. 만약에 체크포인트 파일이 없는 초기상태라면 해당 코드는 비활성화.\n",
        "model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "DsfqDNS2D66E"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = [encoder_input_train, decoder_input_train], \n",
        "          y = decoder_target_train,\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size = BATCH_SIZE, \n",
        "          callbacks = [earlystopping, checkpoint],\n",
        "          epochs = EPOCHS)"
      ],
      "metadata": {
        "id": "YR0mF43ID9gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(hist['epoch'], hist['loss'],\n",
        "           label='Train Loss')\n",
        "  plt.plot(hist['epoch'], hist['val_loss'],\n",
        "           label = 'Val Loss')\n",
        "  plt.ylim([0,5])\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(hist['epoch'], hist['acc'],\n",
        "           label='Train Acc')\n",
        "  plt.plot(hist['epoch'], hist['val_acc'],\n",
        "           label = 'Val Acc')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "LZ-NufNHEOkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더(predict)\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])"
      ],
      "metadata": {
        "id": "o_M3rRVzFawU"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더(predict)\n",
        "\n",
        "# Input Tensors : 이전 시점의 상태를 보관할 텐서\n",
        "decoder_input_h = Input(shape=(HIDDEN_DIM,))\n",
        "decoder_input_c = Input(shape=(HIDDEN_DIM,))\n",
        "\n",
        "decoder_states_inputs = [decoder_input_h, decoder_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "x = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "x, state_h2, state_c2 = dec_lstm(x, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 수정된 디코더\n",
        "attn_layer = att([x, encoder_outputs])\n",
        "decoder_concat = Concatenate(axis=-1)([attn_layer, x])\n",
        "attn_out = dense_tanh(decoder_concat)\n",
        "decoder_outputs = dec_dense(attn_out)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs + [encoder_outputs],\n",
        "    [decoder_outputs] + decoder_states2)"
      ],
      "metadata": {
        "id": "KDw4RB5CFc5U"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    tokens = hug_kor_tok.encode(sentence).tokens\n",
        "\n",
        "    # 입력 문장 토큰 -> 라벨링\n",
        "    enc_input = tokenizer_kor.texts_to_sequences([tokens])\n",
        "\n",
        "    # 입력 문장 라벨링 -> 패딩 \n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, maxlen=MAX_ENC_LEN, padding='post')\n",
        "    encoder_output, states_value = encoder_model.predict(enc_input)\n",
        "\n",
        "    # Decoder input인 <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar2idx['<sos>']\n",
        "\n",
        "    # prediction 시작\n",
        "        # stop_condition이 True가 될 때까지 루프 반복\n",
        "        # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    for t in range(MAX_DEC_LEN):\n",
        "\n",
        "        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value + [encoder_output], verbose = 0)\n",
        "\n",
        "        # 예측 결과를 단어로 변환\n",
        "        result_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        result_word = idx2tar[result_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' ' + result_word\n",
        "\n",
        "        # 현재 시점의 예측 결과 -> 다음 시점의 입력으로 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = result_token_index\n",
        "\n",
        "        # 현재 시점의 상태 ->  다음 시점의 상태로 업데이트\n",
        "        states_value = [h, c]\n",
        "\n",
        "        #  Stop condition <eos>에 도달하면 중단.\n",
        "        if result_word == '<eos>':\n",
        "            break \n",
        "\n",
        "    return decoded_sentence.strip('<eos>')"
      ],
      "metadata": {
        "id": "lmWhpp4VFhXp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = indices[:-n_of_val]\n",
        "test_idx = indices[-n_of_val:]"
      ],
      "metadata": {
        "id": "bFjeSX9cFmFW"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 5\n",
        "np.random.choice(train_idx, n_samples)"
      ],
      "metadata": {
        "id": "rYpRS1BjFncW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data - translate\n",
        "n_samples = 5\n",
        "for idx in np.random.choice(train_idx, n_samples):\n",
        "    test_sentence = hangeul_corpus[idx]\n",
        "    answer_sentence = eng_corpus[idx]\n",
        "    decoded_sentence = translate(test_sentence)\n",
        "\n",
        "    print(\"입력문장 :\", test_sentence)\n",
        "    print(\"정답문장 :\", answer_sentence) \n",
        "    print(\"번역문장 :\", decoded_sentence)\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "ANeHNIcjFjLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install evaluate"
      ],
      "metadata": {
        "id": "FKGpv8dMFuzH"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm \n",
        "\n",
        "n_samples = 50\n",
        "ref_train, pred_train = [], []\n",
        "for idx in tqdm(np.random.choice(test_idx, n_samples)):\n",
        "    ref_train.append(eng_corpus[idx])\n",
        "    pred_train.append(translate(hangeul_corpus[idx]))"
      ],
      "metadata": {
        "id": "N_MSFjdIFxkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ref_train) \n",
        "print(pred_train)"
      ],
      "metadata": {
        "id": "aeRRxY7aFy9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "bleu = evaluate.load(\"bleu\")"
      ],
      "metadata": {
        "id": "ytHwDtGnT4Hj"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu.compute(predictions=pred_train, references=ref_train)"
      ],
      "metadata": {
        "id": "X0lQKZ9GF0Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UIxMhtzjTPM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}